{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNPUXRTSfOf9EwY08Oe0xCc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterphoenix/School-Projects/blob/master/Final_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3oB11g8nggK"
      },
      "source": [
        "Peter Phoenix - 2201735413\n",
        "\n",
        "Natural Language Processing - LA01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jbk_kL4Mw9F6"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv1UyzJzxCwz",
        "outputId": "aa9c1f85-912a-48be-e24e-575e27910245",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "data = {\n",
        "    'Data': ['This was a great movie with a good cast, all of them hitting on all cylinders.', \n",
        "             \"Even if you're a huge Sandler fan, please don't bother with this extremely disappointing comedy!\",\n",
        "             'A movie of outstanding brilliance and a poignant and unusual love story',\n",
        "             'I had the misfortune to watch this rubbish on Sky Cinema Max in a cold winter night',\n",
        "             'I am at a distinct disadvantage here. I have not seen the first two movies in this series', \n",
        "             \"This program is a lot of fun and the title song is so catchy I can't get it out of my head\"],\n",
        "    'Label': ['Positive', 'Negative', 'Positive', 'Negative', 'Negative', 'Positive']\n",
        "}\n",
        "data = pd.DataFrame(data, columns=[\"Data\", \"Label\"])\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                Data     Label\n",
            "0  This was a great movie with a good cast, all o...  Positive\n",
            "1  Even if you're a huge Sandler fan, please don'...  Negative\n",
            "2  A movie of outstanding brilliance and a poigna...  Positive\n",
            "3  I had the misfortune to watch this rubbish on ...  Negative\n",
            "4  I am at a distinct disadvantage here. I have n...  Negative\n",
            "5  This program is a lot of fun and the title son...  Positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EgAehSfy9w3"
      },
      "source": [
        "# A. Data Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQZrz4ZT0I3A"
      },
      "source": [
        "**Case folding**\n",
        "\n",
        "Case folding berfungsi untuk mengubah setiap huruf menjadi huruf kecil.\n",
        "\n",
        "Hal ini dilakukan agar kata yang sama dianggap sebagai satu kesatuan. Jika hal ini tidak dilakukan, kata \"Happy\" dan \"happy\" akan dianggap menjadi 2 buah kata yang berbeda. Tahap ini juga memudahkan dan meningkatkan ketepatan pada langkah selanjutnya."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OegfY64Uzpuz",
        "outputId": "a66a8915-c687-42e6-dd62-1954c010fbc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "# Case folding\n",
        "data['Data'] = data['Data'].str.lower()\n",
        "\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                Data     Label\n",
            "0  this was a great movie with a good cast, all o...  Positive\n",
            "1  even if you're a huge sandler fan, please don'...  Negative\n",
            "2  a movie of outstanding brilliance and a poigna...  Positive\n",
            "3  i had the misfortune to watch this rubbish on ...  Negative\n",
            "4  i am at a distinct disadvantage here. i have n...  Negative\n",
            "5  this program is a lot of fun and the title son...  Positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6bX4IqAzxbO"
      },
      "source": [
        "**Tokenization**\n",
        "\n",
        "Tokenization berfungsi untuk memisahkan text menjadi kata-kata terpisah.\n",
        "\n",
        "Kita juga dapat menghilangkan tanda baca dalam kata tersebut jika kita menggunakan tokenizer yang tepat.\n",
        "\n",
        "Tanda baca dihilangkan karena tanda baca tidak mengarah terhadap kelas tertentu. Jika kita menggunakan word_tokenize biasa, maka kata \"I am happy.\" akan dipecah menjadi \"I\", \"am\", \"happy\", \".\". Namun, jika kita menggunakan RegexpTokenizer, kata \"I am happy.\" akan dipecah menjadi \"I\", \"am\", \"happy\" saja tanpa tanda baca titik."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H091G0BGzBnh",
        "outputId": "86cda310-3444-4371-8dd8-9b2a4e159672",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "# Tokenization\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Jika kita menggunakan word_tokenize biasa dari nltk, masih terdapat tanda baca, oleh karena itu kita menggunakan RegexpTokenizer\n",
        "token = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "data['Data'] = data.apply(lambda row: token.tokenize(row['Data']), axis=1)\n",
        " \n",
        "print(data)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "                                                Data     Label\n",
            "0  [this, was, a, great, movie, with, a, good, ca...  Positive\n",
            "1  [even, if, you, re, a, huge, sandler, fan, ple...  Negative\n",
            "2  [a, movie, of, outstanding, brilliance, and, a...  Positive\n",
            "3  [i, had, the, misfortune, to, watch, this, rub...  Negative\n",
            "4  [i, am, at, a, distinct, disadvantage, here, i...  Negative\n",
            "5  [this, program, is, a, lot, of, fun, and, the,...  Positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI72I2Bv0JHz"
      },
      "source": [
        "**Stop-words removal**\n",
        "\n",
        "Stop-words removal berfungsi untuk menghilangkan kata-kata umum yang tidak memiliki makna tertentu.\n",
        "\n",
        "Hal ini dilakukan karena kata-kata yang tidak memiliki makna tertentu tersebut, seperti kata \"the\" akan merusak hasil model nantinya. Kata \"the\" tersebut tidak mengarah terhadap suatu kelas tertentu, oleh karena itu perlu dihilangkan agar tidak masuk kedalam perhitungan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP9XPo6bzsLM",
        "outputId": "3363f2d1-5bea-454a-915f-06a9eb468790",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "# Stop-words removal\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "data['Data'] = data['Data'].apply(lambda x: [item for item in x if item not in stop_words])\n",
        "\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "                                                Data     Label\n",
            "0     [great, movie, good, cast, hitting, cylinders]  Positive\n",
            "1  [even, huge, sandler, fan, please, bother, ext...  Negative\n",
            "2  [movie, outstanding, brilliance, poignant, unu...  Positive\n",
            "3  [misfortune, watch, rubbish, sky, cinema, max,...  Negative\n",
            "4  [distinct, disadvantage, seen, first, two, mov...  Negative\n",
            "5  [program, lot, fun, title, song, catchy, get, ...  Positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_ux5a890JXC"
      },
      "source": [
        "**Stemming**\n",
        "\n",
        "Stemming berfungsi untuk mengubah suatu kata menjadi bentuk paling sederhana dari kata tersebut, seperti kata  “I”, “am”, “tired” akan diubah menjadi “I”, “am”, “tire”. Stemming menggunakan morpheme, yaitu satuan terkecil dari sebuah bahasa, oleh karena itu kata \"happiness\" jika menggunakan stemming akan diubah menjadi \"happi\", bukan \"happy\".\n",
        "\n",
        "Hal ini dilakukan agar kata seperti happiness dan happy tadi dianggap menjadi sebuah kata yang memiliki arti yang sama. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5JaLYTpzwH1",
        "outputId": "c1dddd85-e7ba-4e2c-f7e4-d7836f83bc8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "# Stemming\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "data['Data'] = data['Data'].apply(lambda x: [LancasterStemmer().stem(y) for y in x])\n",
        "\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                Data     Label\n",
            "0               [gre, movy, good, cast, hit, cylind]  Positive\n",
            "1  [ev, hug, sandl, fan, pleas, both, extrem, dis...  Negative\n",
            "2    [movy, outstand, bril, poign, unus, lov, story]  Positive\n",
            "3  [misfortun, watch, rub, sky, cinem, max, cold,...  Negative\n",
            "4   [distinct, disadv, seen, first, two, movy, sery]  Negative\n",
            "5  [program, lot, fun, titl, song, catchy, get, h...  Positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPHj3WxCz46l"
      },
      "source": [
        "# B. Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llQtyEFgz_FT"
      },
      "source": [
        "import gensim \n",
        "from gensim.models import Word2Vec "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDioSoShCE8T",
        "outputId": "792082a6-8abb-4ef0-bc28-514df5a9a974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "vec = Word2Vec(data['Data'], min_count = 1, window = 3, size = 10)\n",
        "\n",
        "print(vec)\n",
        "print(vec.wv.vectors.shape)\n",
        "print(vec.wv.vocab.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=44, size=10, alpha=0.025)\n",
            "(44, 10)\n",
            "dict_keys(['gre', 'movy', 'good', 'cast', 'hit', 'cylind', 'ev', 'hug', 'sandl', 'fan', 'pleas', 'both', 'extrem', 'disappoint', 'comedy', 'outstand', 'bril', 'poign', 'unus', 'lov', 'story', 'misfortun', 'watch', 'rub', 'sky', 'cinem', 'max', 'cold', 'wint', 'night', 'distinct', 'disadv', 'seen', 'first', 'two', 'sery', 'program', 'lot', 'fun', 'titl', 'song', 'catchy', 'get', 'head'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXTZdqvWDBRY"
      },
      "source": [
        "Dasar pemikiran dari Word2Vec adalah dimana setiap word atau kata dapat diterjemahkan menjadi sebuah vector angka dimana setiap kata berbeda memiliki vector yang berbeda dan setiap kata yang sama memiliki vector yang sama. Tujuan dari diubahnya kata menjadi vector angka adalah agar kita dapat melakukan operasi matematika pada data tersebut.\n",
        "\n",
        "Proses Word2Vec menggunakan proses neural network. Pada awalnya, kita menginisialisasi vector input dari setiap kata. Vector tersebut berupa angka dengan ukuran sesuai dengan size Word2Vec, dimana size diatas adalah 10. Kemudian kita menentukan target dari input tersebut berupa kata yang berada di sebelah kata input, sesuai dengan ukuran window sizenya. Window size yang digunakan diatas adalah 3. Jika kita menggunakan window size = 2, maka cara tersebut dinamakan CBoW atau Continuous Bag of Words, namun lebih dari itu dinamakan dengan skipgram.\n",
        "\n",
        "Setelah menentukan input dan outputnya masing-masing (setiap input dapat memiliki lebih dari 1 output, sesuai dengan window sizenya dan sesuai kemunculan dari kata tersebut), proses neural network akan dijalankan hingga hidden layer yang didapatkan dapat memetakan input menjadi target yang sesuai.\n",
        "\n",
        "Proses yang terjadi dapat dijabarkan sebagai berikut:\n",
        "\n",
        "\n",
        "1.   Input berupa vector angka dari kata tertentu dan target berupa vector angka dari kata yang menjadi target input tersebut.\n",
        "2.   Weight diinisialiasi secara random.\n",
        "3. Proses selanjutnya dijalankan seperti proses neural network biasa, dimana dilakukan perkalian matriks dari layer input menuju hidden ditambah dengan bias (jika ada) kemudian digunakan fungsi aktivasi yang berupa fungsi softmax. \n",
        "4. Langkah tersebut diulangi kembali untuk layer hidden menuju output. \n",
        "5. Kemudian hitung error dari layer output dan hidden. Untuk layer output, error dihitung dengan cara selisih dari output yang didapatkan dengan output yang diinginkan. Untuk layer hidden, error dihitung dengan cara melakukan perkalian matriks antara error layer output dengan weight pada hidden layer. \n",
        "6. Setelah mendapatkan error setiap layer, weight diupdate dengan cara selisih dari weight lama dengan error yang telah dikalikan dengan learning rate.\n",
        "7. Langkah 3 hingga 6 diulangi hingga output yang diinginkan sesuai.\n",
        "\n",
        "Setelah menjadi sesuai, hidden layer tersebutlah yang menjadi output  yang kita inginkan dari proses BPNN ini.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUYhqs3pz_Xl"
      },
      "source": [
        "#C. Review detection model using BPNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1QkEImOMXbm"
      },
      "source": [
        "Output yang digunakan yaitu sesusai dengan output pada soal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjrg_0F30Dq8",
        "outputId": "65e22a21-199d-4ce8-c1a6-81e1a52beec5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "output = data['Label']\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    Positive\n",
            "1    Negative\n",
            "2    Positive\n",
            "3    Negative\n",
            "4    Negative\n",
            "5    Positive\n",
            "Name: Label, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0Y9fECcMfEn"
      },
      "source": [
        "Untuk input, pertama kita perlu mencari banyaknya jumlah kata maksimal dalam satu input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV5rhvdFAkKq",
        "outputId": "b76b8885-3446-4009-9d39-3b7a2b3acfc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "max = 0\n",
        "for i in range(data.shape[0]):\n",
        "  temp = len(data['Data'][i])\n",
        "  if max < temp:\n",
        "    max = temp\n",
        "\n",
        "print(max)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiZVSySLJ0Q4"
      },
      "source": [
        "Karna max kata dalam satu input ada 9, maka jumlah feature yang dibuat yaitu 9*10(ukuran vector Word2Vec) = 90 feature. Dan karena kita mempunyai 6 input berbeda, maka dibuat array berukuran 6x90\n",
        "\n",
        "Input pada BPNN yang akan digunakan berisi feature setiap kata yang ada dalam input, yang berarti setiap input memiliki jumlah kata*10 dan sisanya diisi oleh nilai 0.\n",
        "\n",
        "Contohnya jika 1 input memiliki 2 buah kata, maka kata pertama menempati feature 0-9 dan kata kedua menempati feature 10-19, kemudian sisanya yaitu feature 20-89 berisi nilai 0.\n",
        "\n",
        "Untuk membentuk input seperti itu, kita dapat memanfaatkan fungsi for dengan rincian:\n",
        "\n",
        "1.   for pertama untuk baris inputan.\n",
        "2.   for kedua untuk kata ke-j pada inputan di baris tersebut.\n",
        "3.   for ketiga untuk nilai vector dari kata tersebut.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLW7i0AG_AN7",
        "outputId": "dada97a6-1e3c-46f7-a2c0-15367b06f117",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "input = np.zeros((6, 90), dtype = 'float64')\n",
        "\n",
        "for i in range(data.shape[0]):\n",
        "  ct = 0\n",
        "  for j in range(len(data['Data'][i])):\n",
        "    for k in range(vec.wv.vectors.shape[1]) :\n",
        "      input[i][ct] = vec[data['Data'][i][j]][k]\n",
        "      ct+=1\n",
        "\n",
        "print(input)\n",
        "print(input.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.01872313  0.0495575  -0.03518298 -0.00467991 -0.04263043  0.01422503\n",
            "  -0.03249187  0.02335061  0.02648079  0.04800149  0.02520237  0.01018078\n",
            "  -0.03892053 -0.04625027  0.01383292 -0.04281176  0.02569087  0.0240184\n",
            "  -0.01292509  0.04920362 -0.03439813  0.03221051  0.04698032 -0.01958213\n",
            "   0.0395551  -0.0396603  -0.00830341  0.00916785  0.0292665   0.03528667\n",
            "  -0.03697141  0.03452208 -0.0444804  -0.01940611 -0.00778832 -0.00177132\n",
            "   0.0461552   0.01311467 -0.04455207  0.01568138  0.02631926 -0.01900967\n",
            "  -0.01618009 -0.01314419 -0.0154218  -0.01189275  0.00747666 -0.01804278\n",
            "   0.00950475  0.02393494  0.00112207  0.04929164  0.00013444 -0.00737058\n",
            "  -0.01878775  0.04685513 -0.03331408  0.01648496 -0.0482969   0.03081307\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.        ]\n",
            " [ 0.01983638 -0.04401299 -0.02008773  0.01376719  0.04638673 -0.03078436\n",
            "   0.04578525  0.03847618 -0.00987532  0.01768539  0.01622589  0.02731953\n",
            "   0.03439298  0.0196757  -0.04747345  0.00137003  0.00452781  0.00844482\n",
            "  -0.03416996 -0.03188681  0.00196736 -0.01515502 -0.04651859 -0.02848716\n",
            "  -0.01113342 -0.00221231 -0.01908225  0.00547747 -0.04540478  0.0455119\n",
            "   0.02450005 -0.02968894  0.02847597  0.01224608 -0.04051471  0.01487454\n",
            "  -0.03956768  0.02020453  0.02392169 -0.02735029 -0.01000387  0.04553331\n",
            "   0.04280903 -0.01509081  0.01347981 -0.03865333  0.04834817  0.00921929\n",
            "   0.04035038  0.04020398  0.00627131 -0.0124164   0.04766469  0.0227749\n",
            "   0.03506109  0.00837755 -0.04226623 -0.01357704  0.01792137 -0.04938148\n",
            "  -0.03405419 -0.02670603 -0.01535354  0.02400404 -0.04828354 -0.01844148\n",
            "  -0.02741103  0.01302408 -0.02708408  0.02733556  0.03033992  0.03964966\n",
            "   0.04297929 -0.03303479 -0.00134128  0.01958484  0.04066429 -0.0341908\n",
            "   0.03297342  0.01609703 -0.000357    0.0039056  -0.00141384 -0.01533377\n",
            "  -0.02716565  0.03248338  0.01931694 -0.02452328  0.01391106 -0.03925378]\n",
            " [ 0.02520237  0.01018078 -0.03892053 -0.04625027  0.01383292 -0.04281176\n",
            "   0.02569087  0.0240184  -0.01292509  0.04920362  0.00668714  0.04163096\n",
            "  -0.02862894 -0.01020748 -0.01681334  0.02786812 -0.00986817  0.01012568\n",
            "   0.02588463  0.02062757  0.02756283  0.00112144 -0.03220806  0.04154963\n",
            "  -0.01514328 -0.03504417  0.01099985  0.02462366 -0.01573749 -0.00184892\n",
            "   0.03670888  0.01246416  0.04582292  0.04704602  0.0084885   0.02918239\n",
            "  -0.03307261  0.00885276  0.04620331 -0.00700288 -0.04277101  0.04179604\n",
            "   0.00190289  0.00632828 -0.0271103   0.00941751 -0.00902881  0.02250858\n",
            "  -0.00269067  0.00296173  0.01842375 -0.02352197 -0.02177538  0.00036317\n",
            "   0.0063518  -0.02545935 -0.04379126  0.03379125 -0.00532404 -0.01996136\n",
            "  -0.02693847  0.0090406  -0.04545025  0.03474077  0.01382648 -0.03203945\n",
            "   0.01748564  0.01359828 -0.04450196  0.00695753  0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.        ]\n",
            " [ 0.04871878 -0.01442011  0.00674807 -0.03379068 -0.04300209  0.01615175\n",
            "  -0.00424306 -0.02546071  0.00058518  0.00563384 -0.00924366  0.04822268\n",
            "  -0.04762507 -0.02545354  0.00891326 -0.0049765  -0.02337666 -0.02919091\n",
            "   0.02869495  0.03200607 -0.03381628 -0.01013402  0.01730972  0.04101949\n",
            "  -0.01831411 -0.03746301 -0.0194907   0.04533048 -0.03578748  0.03094903\n",
            "   0.01823023  0.04382196  0.02111482 -0.00702848  0.04300289  0.0369099\n",
            "   0.01968404 -0.04649718 -0.0008108   0.04094439 -0.03702201 -0.02498181\n",
            "  -0.03922626 -0.0397201  -0.02376941 -0.03622454 -0.04456581 -0.0186978\n",
            "   0.03476726 -0.04427077  0.03967059 -0.00083321  0.03581033 -0.00517952\n",
            "   0.0047489   0.02070687  0.02320009  0.00526318  0.00207114  0.01196326\n",
            "  -0.00173145  0.0068871  -0.00657952 -0.00259681 -0.03190676  0.00637725\n",
            "   0.0183905   0.0378526  -0.01979019 -0.01591788 -0.04817116 -0.04718548\n",
            "   0.00590282 -0.01373178 -0.00492722 -0.02975411  0.04357715 -0.01024923\n",
            "   0.03109433 -0.04097985  0.00976661 -0.04231895 -0.0276158  -0.01551802\n",
            "  -0.04699783 -0.0418371   0.00302131 -0.00663761 -0.01285954 -0.00068939]\n",
            " [-0.03717062 -0.0175599  -0.04176316  0.00650032 -0.03593741 -0.01927895\n",
            "  -0.01452134  0.00133187 -0.04383887  0.01339259 -0.02460575  0.03298404\n",
            "   0.00331686  0.03521061  0.03753429 -0.0279642  -0.01650251  0.00248462\n",
            "   0.01489117  0.03568728 -0.00224556  0.025556    0.03409052 -0.02086127\n",
            "   0.01343185 -0.00192321  0.01687013  0.01511018  0.03682264 -0.00936455\n",
            "   0.04555351 -0.03754165 -0.02562049  0.02632845  0.04393378  0.00945448\n",
            "  -0.01661911  0.03145451 -0.0135909  -0.00447729  0.03906136  0.02632199\n",
            "   0.01130854  0.04557789  0.00331274  0.00415735 -0.04605622 -0.04392327\n",
            "   0.00318418  0.0149247   0.02520237  0.01018078 -0.03892053 -0.04625027\n",
            "   0.01383292 -0.04281176  0.02569087  0.0240184  -0.01292509  0.04920362\n",
            "   0.03033781 -0.02430715 -0.01188788  0.03855035  0.03202725  0.04256716\n",
            "  -0.02505727 -0.00569121  0.04340483  0.02191162  0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.        ]\n",
            " [ 0.03128386  0.02460868 -0.0407073   0.01886122 -0.02296253 -0.03314606\n",
            "  -0.01088393  0.02692778  0.00228196  0.02288284 -0.02286937 -0.03975659\n",
            "  -0.03845007  0.00263085  0.00378614  0.00685388  0.01369619 -0.01421352\n",
            "   0.01290638 -0.02146154  0.04300559  0.01793709  0.04578577 -0.00548049\n",
            "   0.04353528 -0.03773473  0.04601932  0.01624495 -0.0474897  -0.04235527\n",
            "   0.03410607  0.01807733  0.00682482  0.03185917 -0.0082154   0.03955804\n",
            "  -0.01464467  0.01524201  0.0334444  -0.03382947  0.03169746 -0.03284962\n",
            "  -0.04506425  0.04552573  0.04015114 -0.02496409 -0.03836546 -0.04006493\n",
            "  -0.00474419 -0.00463032  0.03354852 -0.0139268   0.00545315 -0.0132703\n",
            "   0.02163064  0.01406175 -0.04403619 -0.04323785  0.0199017  -0.00951017\n",
            "   0.04986657  0.04407604  0.03962858  0.00978921 -0.01447153 -0.021053\n",
            "  -0.02498974 -0.03627696  0.01471475 -0.00763965 -0.04715252 -0.02023878\n",
            "   0.04059118 -0.00309241  0.03882749  0.03215216 -0.00536381 -0.04758114\n",
            "   0.02844936 -0.01176385  0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.        ]]\n",
            "(6, 90)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiFRV5ojzXIB"
      },
      "source": [
        "**BPNN - Inisialisasi Data**\n",
        "\n",
        "*   Sebelum melakukan proses training didefinisikan terlebih dahulu fungsi aktivasi berupa fungsi sigmoid seperti dibawah.\n",
        "*   Masukkan input dan output dataset\n",
        "*   Input data dinormalisasi menggunakan MinMaxScaler karena hasil yang didapatkan setelah dilakukan Word2Vec sangatlah kecil (dapat dilihat diatas bahwa input berisi nilai 0.0006..., 0.0011..., dan seterusnya).\n",
        "*   Output data dinormalisasi menggunakan OneHotEncoder karena output masih berupa string, harus diubah menjadi numeric. Dapat dilihat dibawah bahwa class 1 adalah positive dan class 0 adalah negative\n",
        "*   Inisialisasi weight dan bias secara random, serta inisialisasi learning rate (alpha). Disini kita menggunakan 4 buah hidden layer.\n",
        "*   Bagi dataset untuk training dan testing menggunakan train_test_split\n",
        "*   np.argmax yang digunakan pada target_train dan target_test hanya untuk saat menghitung akurasi, recall, dan presisi dikarenakan setelah dilakukan OneHotEncoder pada output, hasil yang didapatkan adalah array 6x2 yang berupa kemungkinan suatu input berapa di kelas output tertentu. Contohnya nilai [0.7 0.8] berarti input tersebut memiliki 70% kemungkinan untuk berada di kelas 0 dan 80% kemungkinan berada di kelas 1. np.argmax mengubah array tersebut kembali menjadi 6x1 dengan cara memilih nilai terbesar dan mengembalikan indexnya.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ON1XP-x2pkUb",
        "outputId": "9a1f0849-3a2c-470c-bbc8-f478a1c6c305",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Sigmoid function\n",
        "def nonlin(x,deriv=False):\n",
        "  if(deriv==True):\n",
        "    return x*(1-x)\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "# Input dataset\n",
        "data = input\n",
        "\n",
        "# Output dataset\n",
        "target = output.values.reshape((len(output), -1))\n",
        "print(\"Original Target:\")\n",
        "print(target)\n",
        "data = MinMaxScaler().fit_transform(data)\n",
        "target = OneHotEncoder(sparse = False).fit_transform(target)\n",
        "print(\"Target After One Hot Encoding:\")\n",
        "print(target)\n",
        "print(\"Output yang dimaksud setelah One Hot Encoding\")\n",
        "print(np.argmax(target, axis = 1))\n",
        "\n",
        "np.random.seed(1)\n",
        "alpha = 0.001\n",
        "\n",
        "# Inisialisasi weight dan bias\n",
        "syn0 = np.random.uniform(size=(90,4))\n",
        "bias0 =np.random.uniform(size=(1,4))\n",
        "syn1 = np.random.uniform(size=(4,2))\n",
        "bias1 = np.random.uniform(size=(1,2))\n",
        "\n",
        "X, feature_test, y, target_test = train_test_split(data, target, test_size=0.3)\n",
        "\n",
        "target_train = np.argmax(y, axis = 1)\n",
        "target_test = np.argmax(target_test, axis = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Target:\n",
            "[['Positive']\n",
            " ['Negative']\n",
            " ['Positive']\n",
            " ['Negative']\n",
            " ['Negative']\n",
            " ['Positive']]\n",
            "Target After One Hot Encoding:\n",
            "[[0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]]\n",
            "Output yang dimaksud setelah One Hot Encoding\n",
            "[1 0 1 0 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtbNTKM-zVEG"
      },
      "source": [
        "**BPNN - Training Data**\n",
        "\n",
        "*   Training dilakukan dengan epoch sebesar 5000.\n",
        "*   Tipe training yang digunakan adalah batch learning, dimana setiap epoch menggunakan seluruh input sekaligus.\n",
        "*   Forward propagation dilakukan dengan cara mengalikan matrix yang ada dari input ke hidden ditambahkan dengan bias kemudian dilakukan fungsi sigmoid, setelah itu langkah tersebut diulangi untuk hidden ke output.\n",
        "*   Pada bagian backpropagation, kita menghitung error dan delta dimana error adalah selisih dari output yang didapatkan dengan target. Error pada l2 didapatkan dengan mengurangi l2 dengan output target, namun error pada l1 didapatkan dengan cara mengalikan error pada l2 dengan weight l1. Delta adalah error yang dikalikan dengan error itu sendiri yang sudah digunakan fungsi aktivasi (dalam kode ini berupa fungsi sigmoid).\n",
        "*   Update weight dan bias \n",
        "*   3 langkah diatas diulangi sebanyak 5000 kali (sebanyak jumlah epoch).\n",
        "*   Setelah selesai, hasil yang kita inginkan adalah weight pada l1 (layer 1 atau hidden layer) yang kemudian akan dilakukan forward propagation sekali lagi pada data yang ingin didapatkan hasilnya.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUVz5aw1zVQE"
      },
      "source": [
        "for idx in range(5000):\n",
        "  # Forward Propagation --> Batch Learning\n",
        "  l1 = nonlin(np.dot(X,syn0) + bias0)\n",
        "  l2 = nonlin(np.dot(l1,syn1) + bias1)\n",
        "\n",
        "  # Backpropagation\n",
        "  l2_error = y - l2\n",
        "  l2_delta = l2_error * nonlin(l2, deriv = True)\n",
        "  l1_error = l2_delta.dot(syn1.T)\n",
        "  l1_delta = l1_error * nonlin(l1, deriv = True)\n",
        "\n",
        "  # Update Weight dan Bias\n",
        "  syn1 += l1.T.dot(l2_delta) * alpha\n",
        "  bias1 += np.sum(l2_delta,axis=0,keepdims=True) * alpha\n",
        "  syn0 += X.T.dot(l1_delta) * alpha\n",
        "  bias0 += np.sum(l1_delta,axis=0,keepdims=True) * alpha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lhuyX7o9B8E"
      },
      "source": [
        "**BPNN - Output**\n",
        "\n",
        "*   Untuk mendapatkan output yang diinginkan, lakukan forward propagation sekali lagi pada data yang ingin didapatkan outputnya.\n",
        "*   Output yang didapatkan pada l2 masih berupa float, oleh karena itu perlu digunakan argmax untuk mendapatkan hasilnya.\n",
        "*   Semua output terklasifikasi menjadi kelas 1, hal ini dapat dikarenakan data yang digunakan sangatlah sedikit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhD9kIAY9AQW",
        "outputId": "10daad6b-6823-403b-ec3d-669d333708e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"Weight:\")\n",
        "print(syn0)\n",
        "\n",
        "l1 = nonlin(np.dot(feature_test,syn0) + bias0)\n",
        "l2 = nonlin(np.dot(l1,syn1) + bias1)\n",
        "\n",
        "result = np.argmax(l2, axis = 1)\n",
        "print(\"Test result: \", result)\n",
        "\n",
        "l1_train = nonlin(np.dot(X,syn0) + bias0)\n",
        "l2_train = nonlin(np.dot(l1_train,syn1) + bias1)\n",
        "\n",
        "result_train = np.argmax(l2_train, axis = 1)\n",
        "print(\"Train result: \", result_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weight:\n",
            "[[4.17022004e-01 7.20324493e-01 1.14374480e-04 3.02332573e-01]\n",
            " [1.46755891e-01 9.23385947e-02 1.86260211e-01 3.45560727e-01]\n",
            " [3.96767474e-01 5.38816734e-01 4.19194514e-01 6.85219500e-01]\n",
            " [2.04452250e-01 8.78117436e-01 2.73875931e-02 6.70467510e-01]\n",
            " [4.17304802e-01 5.58689828e-01 1.40386939e-01 1.98101489e-01]\n",
            " [8.00744568e-01 9.68261576e-01 3.13424178e-01 6.92322616e-01]\n",
            " [8.76389152e-01 8.94606663e-01 8.50442112e-02 3.90547832e-02]\n",
            " [1.69830420e-01 8.78142503e-01 9.83468338e-02 4.21107625e-01]\n",
            " [9.57889530e-01 5.33165285e-01 6.91877114e-01 3.15515631e-01]\n",
            " [6.86500928e-01 8.34625672e-01 1.82882773e-02 7.50144315e-01]\n",
            " [9.88861089e-01 7.48165654e-01 2.80443992e-01 7.89279328e-01]\n",
            " [1.03226006e-01 4.47893526e-01 9.08595503e-01 2.93614148e-01]\n",
            " [2.87775339e-01 1.30028572e-01 1.93669579e-02 6.78835533e-01]\n",
            " [2.11628116e-01 2.65546659e-01 4.91573159e-01 5.33625451e-02]\n",
            " [5.74117605e-01 1.46728575e-01 5.89305537e-01 6.99758360e-01]\n",
            " [1.02334429e-01 4.14055988e-01 6.94400158e-01 4.14179270e-01]\n",
            " [4.99534589e-02 5.35896406e-01 6.63794645e-01 5.14889112e-01]\n",
            " [9.44594756e-01 5.86555040e-01 9.03401915e-01 1.37474704e-01]\n",
            " [1.39276347e-01 8.07391289e-01 3.97676837e-01 1.65354197e-01]\n",
            " [9.27508580e-01 3.47765860e-01 7.50812103e-01 7.25997985e-01]\n",
            " [8.83306091e-01 6.23672207e-01 7.50942434e-01 3.48898342e-01]\n",
            " [2.69927892e-01 8.95886218e-01 4.28091190e-01 9.64840047e-01]\n",
            " [6.63441498e-01 6.21695720e-01 1.14745973e-01 9.49489259e-01]\n",
            " [4.49912133e-01 5.78389614e-01 4.08136802e-01 2.37026980e-01]\n",
            " [9.03379521e-01 5.73679487e-01 2.87032702e-03 6.17144914e-01]\n",
            " [3.26644902e-01 5.27058102e-01 8.85942099e-01 3.57269760e-01]\n",
            " [9.08535151e-01 6.23360116e-01 1.58212428e-02 9.29437234e-01]\n",
            " [6.90896917e-01 9.97322850e-01 1.72340508e-01 1.37135750e-01]\n",
            " [9.32595463e-01 6.96818161e-01 6.60001727e-02 7.55463053e-01]\n",
            " [7.53876188e-01 9.23024535e-01 7.11524758e-01 1.24270962e-01]\n",
            " [1.98801337e-02 2.62109868e-02 2.83064878e-02 2.46211068e-01]\n",
            " [8.60027948e-01 5.38831064e-01 5.52821978e-01 8.42030892e-01]\n",
            " [1.24173315e-01 2.79183679e-01 5.85759271e-01 9.69595748e-01]\n",
            " [5.61030219e-01 1.86472893e-02 8.00632673e-01 2.32974274e-01]\n",
            " [8.07105195e-01 3.87860644e-01 8.63541854e-01 7.47121643e-01]\n",
            " [5.56240234e-01 1.36455226e-01 5.99176892e-02 1.21343456e-01]\n",
            " [4.45518784e-02 1.07494129e-01 2.25709338e-01 7.12988980e-01]\n",
            " [5.59716982e-01 1.25559801e-02 7.19742797e-02 9.67276330e-01]\n",
            " [5.68100462e-01 2.03293235e-01 2.52325744e-01 7.43825854e-01]\n",
            " [1.95429481e-01 5.81358927e-01 9.70019989e-01 8.46828801e-01]\n",
            " [2.39847759e-01 4.93769714e-01 6.19955718e-01 8.28980900e-01]\n",
            " [1.56791395e-01 1.85762022e-02 7.00221437e-02 4.86345111e-01]\n",
            " [6.06329462e-01 5.68851437e-01 3.17362409e-01 9.88616154e-01]\n",
            " [5.79745219e-01 3.80141173e-01 5.50948219e-01 7.45334431e-01]\n",
            " [6.69232893e-01 2.64919558e-01 6.63348344e-02 3.70084198e-01]\n",
            " [6.29717507e-01 2.10174010e-01 7.52755554e-01 6.65364814e-02]\n",
            " [2.60315099e-01 8.04754564e-01 1.93434283e-01 6.39460881e-01]\n",
            " [5.24670309e-01 9.24807970e-01 2.63296770e-01 6.59610907e-02]\n",
            " [7.35065963e-01 7.72178029e-01 9.07815852e-01 9.31972069e-01]\n",
            " [1.39515730e-02 2.34362086e-01 6.16778357e-01 9.49016321e-01]\n",
            " [9.50176119e-01 5.56653188e-01 9.15606349e-01 6.41566209e-01]\n",
            " [3.90007714e-01 4.85990667e-01 6.04310483e-01 5.49547921e-01]\n",
            " [9.26181426e-01 9.18733436e-01 3.94875613e-01 9.63262528e-01]\n",
            " [1.73955667e-01 1.26329519e-01 1.35079158e-01 5.05662166e-01]\n",
            " [2.15248052e-02 9.47970211e-01 8.27115471e-01 1.50189807e-02]\n",
            " [1.76196255e-01 3.32063574e-01 1.30996845e-01 8.09490692e-01]\n",
            " [3.44736652e-01 9.40107482e-01 5.82014180e-01 8.78831984e-01]\n",
            " [8.44734445e-01 9.05392319e-01 4.59880266e-01 5.46346816e-01]\n",
            " [7.98603591e-01 2.85718852e-01 4.90253522e-01 5.99110308e-01]\n",
            " [1.55332754e-02 5.93481408e-01 4.33676349e-01 8.07360529e-01]\n",
            " [3.15244803e-01 8.92888708e-01 5.77857215e-01 1.84010202e-01]\n",
            " [7.87929234e-01 6.12031177e-01 5.39092719e-02 4.20193680e-01]\n",
            " [6.79068836e-01 9.18601778e-01 4.02024733e-04 9.76759149e-01]\n",
            " [3.76580315e-01 9.73783538e-01 6.04716101e-01 8.28845808e-01]\n",
            " [5.74711505e-01 6.28076198e-01 2.85576282e-01 5.86833341e-01]\n",
            " [7.50021764e-01 8.58313836e-01 7.55082188e-01 6.98057248e-01]\n",
            " [8.64479430e-01 3.22680997e-01 6.70788790e-01 4.50873936e-01]\n",
            " [3.82102752e-01 4.10811350e-01 4.01479583e-01 3.17383946e-01]\n",
            " [6.21919368e-01 4.30247271e-01 9.73802078e-01 6.77800891e-01]\n",
            " [1.98569888e-01 4.26701009e-01 3.43346240e-01 7.97638804e-01]\n",
            " [8.79998289e-01 9.03841956e-01 6.62719812e-01 2.70208262e-01]\n",
            " [2.52366702e-01 8.54897943e-01 5.27714646e-01 8.02161084e-01]\n",
            " [5.72488517e-01 7.33142525e-01 5.19011627e-01 7.70883910e-01]\n",
            " [5.68857991e-01 4.65709879e-01 3.42688908e-01 6.82093484e-02]\n",
            " [3.77924179e-01 7.96260777e-02 9.82817114e-01 1.81612851e-01]\n",
            " [8.11858698e-01 8.74961645e-01 6.88413252e-01 5.69494413e-01]\n",
            " [1.60971437e-01 4.66880023e-01 3.45172051e-01 2.25039958e-01]\n",
            " [5.92511869e-01 3.12269838e-01 9.16305553e-01 9.09635525e-01]\n",
            " [2.57118294e-01 1.10891301e-01 1.92962732e-01 4.99584171e-01]\n",
            " [7.28585668e-01 2.08194438e-01 2.48033558e-01 8.51671875e-01]\n",
            " [4.15848718e-01 6.16685067e-01 2.33666139e-01 1.01967259e-01]\n",
            " [5.15857017e-01 4.77140987e-01 1.52671644e-01 6.21806232e-01]\n",
            " [5.44010119e-01 6.54137347e-01 1.44545540e-01 7.51527817e-01]\n",
            " [2.22049140e-01 5.19351824e-01 7.85296028e-01 2.23304280e-02]\n",
            " [3.24362460e-01 8.72922376e-01 8.44709608e-01 5.38440593e-01]\n",
            " [8.66608274e-01 9.49805991e-01 8.26406998e-01 8.54115444e-01]\n",
            " [9.87434018e-02 6.51304332e-01 7.03516988e-01 6.10240813e-01]\n",
            " [7.99615262e-01 3.45712198e-02 7.70238734e-01 7.31728601e-01]\n",
            " [2.59698393e-01 2.57069299e-01 6.32303317e-01 3.45297462e-01]\n",
            " [7.96588678e-01 4.46146232e-01 7.82749414e-01 9.90471784e-01]]\n",
            "Test result:  [1 1]\n",
            "Train result:  [1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLehO9qH0Dyu"
      },
      "source": [
        "#D. Performance evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wyUsMo0x3a_",
        "outputId": "898e9fc2-e0fd-4873-ab7d-71ceb3f04e2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "confusion = metrics.confusion_matrix(target_test, result)\n",
        "print(\"Confusion Matrix Test Dataset:\")\n",
        "print(confusion)\n",
        "\n",
        "accuracy = metrics.accuracy_score(target_test, result)\n",
        "print (\"Accuracy Test Dataset :{}%\".format(accuracy*100))\n",
        "\n",
        "precision = metrics.precision_score(target_test, result, zero_division=1)\n",
        "print(\"Precision Test Dataset :{}%\".format(precision*100))\n",
        "\n",
        "recall = metrics.recall_score(target_test, result, zero_division=1)\n",
        "print(\"Recall Test Dataset :{}%\".format(recall*100))\n",
        "\n",
        "\n",
        "\n",
        "confusion = metrics.confusion_matrix(target_train, result_train)\n",
        "print(\"\\n\\nConfusion Matrix Train Dataset:\")\n",
        "print(confusion)\n",
        "\n",
        "accuracy = metrics.accuracy_score(target_train, result_train)\n",
        "print (\"Accuracy Train Dataset :{}%\".format(accuracy*100))\n",
        "\n",
        "precision = metrics.precision_score(target_train, result_train, zero_division=1)\n",
        "print(\"Precision Train Dataset :{}%\".format(precision*100))\n",
        "\n",
        "recall = metrics.recall_score(target_train, result_train, zero_division=1)\n",
        "print(\"Recall Train Dataset :{}%\".format(recall*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix Test Dataset:\n",
            "[[0 2]\n",
            " [0 0]]\n",
            "Accuracy Test Dataset :0.0%\n",
            "Precision Test Dataset :0.0%\n",
            "Recall Test Dataset :100.0%\n",
            "\n",
            "\n",
            "Confusion Matrix Train Dataset:\n",
            "[[0 1]\n",
            " [0 3]]\n",
            "Accuracy Train Dataset :75.0%\n",
            "Precision Train Dataset :75.0%\n",
            "Recall Train Dataset :100.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYPOKjjuDucH"
      },
      "source": [
        "Hasil yang didapatkan adalah accuracy 0%, precision 0%, dan recall 100% untuk test dataset yang hanya memiliki 2 buah data, dan accuracy 75%, precision 75%, dan recall 100% untuk train dataset yang memiliki 4 buah data.\n",
        "\n",
        "Hasil ini sangatlah tidak baik, dikarenakan jumlah dataset yang sangat sedikit baik untuk training maupun untuk testing, oleh karena itu proses traning tidak berjalan dengan baik, dan data akan lebih cenderung mengarah pada target yang memiliki output lebih banyak saat training (berdasarkan data diatas, dapat dilihat pada train dataset memiliki 3 buah data di kelas 1(positive) dan 1 buah data dengan kelas 0(negative), namun pada test dataset kedua data adalah kelas 0(negative)).\n",
        "\n",
        "Hal lain yang mempengaruhi hasilnya adalah dimana setiap dapat memiliki input kata yang berbeda, oleh karena itu kata yang telah di training tidak digunakan saat testing, begitu juga sebaliknya kata yang muncul saat testing belum dilakukan training."
      ]
    }
  ]
}