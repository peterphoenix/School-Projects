{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project ANN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "YtfTOhVh2JKY",
        "Tj2Qytol43XN",
        "GVeQmyM5u89W"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterphoenix/School-Projects/blob/master/Project_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1aFeikstEL0"
      },
      "source": [
        "# Project ANN LAB - BB01\n",
        "\n",
        "Peter Phoenix - 2201735413\n",
        "\n",
        "Richard Sudaryono A. - 2201771072 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT9bfrElilEa"
      },
      "source": [
        "# TODO:\n",
        "\n",
        "nanti bagian ini dihapus sebelum dikumpul"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS8DcaoSjIUj",
        "outputId": "47ca15dd-300c-4a4b-f4bf-e2ab8e3b06de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijVaPUOtjdF_",
        "outputId": "c68a58d1-73e6-46fa-e4f5-0c25412adce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "!wget \"http://datasetprojectlabann.surge.sh/E202-COMP7117-TD01-00 - clustering.csv\"\n",
        "!wget \"http://datasetprojectlabann.surge.sh/E202-COMP7117-TD01-00 - classification.csv\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-12 03:40:35--  http://datasetprojectlabann.surge.sh/E202-COMP7117-TD01-00%20-%20clustering.csv\n",
            "Resolving datasetprojectlabann.surge.sh (datasetprojectlabann.surge.sh)... 138.197.235.123\n",
            "Connecting to datasetprojectlabann.surge.sh (datasetprojectlabann.surge.sh)|138.197.235.123|:80... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2020-06-12 03:40:36 ERROR 404: Not Found.\n",
            "\n",
            "--2020-06-12 03:40:38--  http://datasetprojectlabann.surge.sh/E202-COMP7117-TD01-00%20-%20classification.csv\n",
            "Resolving datasetprojectlabann.surge.sh (datasetprojectlabann.surge.sh)... 138.197.235.123\n",
            "Connecting to datasetprojectlabann.surge.sh (datasetprojectlabann.surge.sh)|138.197.235.123|:80... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2020-06-12 03:40:38 ERROR 404: Not Found.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoS-H2ght7TU"
      },
      "source": [
        "# yang bakal dikumpul"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgufZPXot9iE"
      },
      "source": [
        "## clustering.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfroRVcbt_ea",
        "outputId": "1836e379-30b0-4070-b581-99e959993f5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "data_clustering = pd.read_csv('E202-COMP7117-TD01-00 - clustering.csv')\n",
        " \n",
        "# Feature Selection\n",
        "new_clustering = pd.DataFrame(columns = ['Special Day Rate', 'Visitor Type', 'Weekend', 'Product Related Duration', 'Exit Rates'])\n",
        "\n",
        "new_clustering['Special Day Rate'] = data_clustering['SpecialDay'].map({'LOW': 0, 'NORMAL': 1, 'HIGH': 2})\n",
        "new_clustering['Visitor Type'] = data_clustering['VisitorType'].map({'Other': 0, 'New_Visitor': 1, 'Returning_Visitor': 2})\n",
        "new_clustering['Weekend'] = data_clustering['Weekend'].map({False: 0, True: 1})\n",
        "new_clustering['Product Related Duration'] = data_clustering['ProductRelated_Duration']\n",
        "new_clustering['Exit Rates'] = data_clustering['ExitRates']\n",
        "\n",
        "# Feature Extraction\n",
        "\n",
        "def calc_mean(dataset):\n",
        "    return tf.reduce_mean(dataset, axis = 0)\n",
        "\n",
        "def normalize_data(dataset, mean):\n",
        "    return dataset - mean\n",
        "\n",
        "def apply_pca(normalized_dataset, n):\n",
        "    pca = PCA(n_components = n)\n",
        "    pca = pca.fit(normalized_dataset)\n",
        "    return pca.transform(normalized_dataset)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    clustering_mean = sess.run(calc_mean(new_clustering))\n",
        "    \n",
        "clustering_norm = normalize_data(new_clustering, clustering_mean)\n",
        "clustering_pca = apply_pca(clustering_norm, 3)\n",
        "\n",
        "class SOM:\n",
        "    def __init__(self, width, height, input_dimension):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.input_dimension = input_dimension\n",
        "\n",
        "        self.weight = tf.Variable(tf.random_normal([width * height, input_dimension]))\n",
        "        self.input = tf.placeholder(tf.float32, [input_dimension])\n",
        "\n",
        "        self.location = [tf.to_float([y,x]) for y in range(height) for x in range(width)]\n",
        "\n",
        "        self.bmu = self.getBMU()\n",
        "\n",
        "        self.update_weight = self.update_neigbours()\n",
        "\n",
        "    def getBMU(self):\n",
        "        square_distance = tf.square(self.input - self.weight)\n",
        "        distance = tf.sqrt(tf.reduce_sum(square_distance, axis=1))\n",
        "\n",
        "        bmu_index = tf.argmin(distance)\n",
        "\n",
        "        bmu_position = tf.to_float([tf.div(bmu_index,self.width), tf.mod(bmu_index, self.width)])\n",
        "        return bmu_position\n",
        "\n",
        "    def update_neigbours(self):\n",
        "        learning_rate = 0.8\n",
        "\n",
        "        sigma = tf.to_float(tf.maximum(self.width, self.height) / 2)\n",
        "\n",
        "        square_difference = tf.square(self.bmu - self.location)\n",
        "        distance = tf.sqrt(tf.reduce_sum(square_difference,axis=1))\n",
        "\n",
        "        NS = tf.exp(tf.div(tf.negative(tf.square(distance)), 2 * tf.square(sigma)))\n",
        "\n",
        "        rate = NS * learning_rate\n",
        "\n",
        "        rate_stacked = tf.stack([tf.tile(tf.slice(rate,[i],[1]), [self.input_dimension]) \n",
        "            for i in range(self.width * self.height)])\n",
        "\n",
        "        new_weight = self.weight + rate_stacked * (self.input - self.weight)\n",
        "\n",
        "        return tf.assign(self.weight, new_weight)\n",
        "\n",
        "    def train(self, dataset, epoch):\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "\n",
        "            for i in range(epoch+1):\n",
        "                for data in dataset:\n",
        "                    dictionary = {\n",
        "                        self.input : data\n",
        "                    }\n",
        "\n",
        "                    sess.run(self.update_weight,feed_dict=dictionary)\n",
        "\n",
        "            location = sess.run(self.location)\n",
        "            weight = sess.run(self.weight)\n",
        "\n",
        "            clusters = [[] for i in range(self.height)]\n",
        "\n",
        "            for i, loc in enumerate(location):\n",
        "                clusters[int(loc[0])].append(weight[i])\n",
        "\n",
        "            self.clusters = clusters\n",
        "\n",
        "input_dimension = len(clustering_pca[0])\n",
        "epoch = 5000\n",
        "\n",
        "som = SOM(8, 8, input_dimension)\n",
        "\n",
        "som.train(clustering_pca,epoch)\n",
        "plt.imshow(som.clusters)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-4e8d5b1c8212>:45: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From <ipython-input-3-4e8d5b1c8212>:57: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKvblm55t_3_"
      },
      "source": [
        "## classification.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmhjMdf9uBp6",
        "outputId": "21a94bed-420d-4596-ab27-654aa1778049",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data_classification = pd.read_csv('E202-COMP7117-TD01-00 - classification.csv')\n",
        "\n",
        "# Feature Selection\n",
        "new_classification = pd.DataFrame(columns = ['Volatile Acidity', 'Chlorides', 'Free Sulfur Dioxide', 'Total Sulfur Dioxide', 'Density', 'pH', 'Sulphates', 'Alcohol'])\n",
        "new_classification['Volatile Acidity'] = data_classification['volatile acidity']\n",
        "new_classification['Chlorides'] = data_classification['chlorides']\n",
        "new_classification['Free Sulfur Dioxide'] = data_classification['free sulfur dioxide'].map({'Low': 1, 'Medium': 2, 'High': 3}).fillna(0).astype(int)\n",
        "new_classification['Total Sulfur Dioxide'] = data_classification['total sulfur dioxide']\n",
        "new_classification['Density'] = data_classification['density'].map({'Very High': 0, 'Low':1, 'Medium': 2, 'High': 3})\n",
        "new_classification['pH'] = data_classification['pH'].map({'Very Acidic': 1, 'Normal': 2, 'Very Basic': 3}).fillna(0).astype(int)\n",
        "new_classification['Sulphates'] = data_classification['sulphates']\n",
        "new_classification['Alcohol'] = data_classification['alcohol']\n",
        "\n",
        "# Feature Extraction\n",
        "def calc_mean(dataset):\n",
        "    return tf.reduce_mean(dataset, axis = 0)\n",
        "\n",
        "def normalize_data(dataset, mean):\n",
        "    return dataset - mean\n",
        "\n",
        "def apply_pca(normalized_dataset, n):\n",
        "    pca = PCA(n_components = n)\n",
        "    pca = pca.fit(normalized_dataset)\n",
        "    return pca.transform(normalized_dataset)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    classification_mean = sess.run(calc_mean(new_classification))\n",
        "\n",
        "classification_norm = normalize_data(new_classification, classification_mean)\n",
        "classification_pca = apply_pca(classification_norm, 4)\n",
        "\n",
        "input_data = MinMaxScaler().fit_transform(classification_pca)\n",
        "output_data = OneHotEncoder(sparse=False).fit_transform(data_classification[['quality']])\n",
        "\n",
        "# Training\n",
        "layer = {\n",
        "    \"input\": 4,\n",
        "    \"hidden\": 30,\n",
        "    \"output\": 5\n",
        "}\n",
        "\n",
        "weight = {\n",
        "    \"input_to_hidden\": tf.Variable(tf.random_normal([layer[\"input\"], layer[\"hidden\"]])),\n",
        "    \"hidden_to_output\": tf.Variable(tf.random_normal([layer[\"hidden\"], layer[\"output\"]]))\n",
        "}\n",
        "\n",
        "bias = {\n",
        "    \"input_to_hidden\": tf.Variable(tf.random_normal([layer[\"hidden\"]])),\n",
        "    \"hidden_to_output\": tf.Variable(tf.random_normal([layer[\"output\"]]))\n",
        "}\n",
        "\n",
        "input_placeholder = tf.placeholder(tf.float32, [None, layer[\"input\"]])\n",
        "output_placeholder = tf.placeholder(tf.float32, [None, layer[\"output\"]])\n",
        "\n",
        "def feed_forward(data):\n",
        "    y = tf.matmul(data, weight[\"input_to_hidden\"]) + bias[\"input_to_hidden\"]\n",
        "    y2 = tf.nn.sigmoid(y)\n",
        "\n",
        "    z = tf.matmul(y2, weight[\"hidden_to_output\"]) + bias[\"hidden_to_output\"]\n",
        "    z2 = tf.nn.sigmoid(z)\n",
        "\n",
        "    return z2\n",
        "\n",
        "output = feed_forward(input_placeholder)\n",
        "\n",
        "epoch = 5000\n",
        "alpha = 0.9\n",
        "\n",
        "loss = tf.reduce_mean(0.5 * (output_placeholder - output) ** 2)\n",
        "optimizer = tf.train.GradientDescentOptimizer(alpha)\n",
        "train = optimizer.minimize(loss)\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "prev_validation_error = 0\n",
        "\n",
        "input_train, input_validation, output_train, output_validation = train_test_split(input_data, output_data, test_size=0.2)\n",
        "input_train, input_test, output_train, output_test = train_test_split(input_train, output_train, test_size=0.125)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for i in range(1, epoch+1):\n",
        "\n",
        "        train_dict = {\n",
        "            input_placeholder: input_train,\n",
        "            output_placeholder: output_train\n",
        "        }\n",
        "\n",
        "        sess.run(train, feed_dict=train_dict)\n",
        "        error_train = sess.run(loss, feed_dict=train_dict)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(\"Epoch = {}, Current Error = {}\".format(i, error_train))\n",
        "\n",
        "        if i % 500 == 0:\n",
        "            validation_dict = {\n",
        "                input_placeholder: input_validation,\n",
        "                output_placeholder: output_validation\n",
        "            }\n",
        "\n",
        "            error_validation = sess.run(loss, feed_dict=validation_dict)\n",
        "\n",
        "            print(\"Epoch = {}, Validation Error = {}\".format(i, error_validation))\n",
        "\n",
        "            if i == 500 or error_validation < prev_validation_error:\n",
        "                prev_validation_error = error_validation\n",
        "                saver.save(sess, \"model/model.ckpt\")\n",
        "\n",
        "    accuracy = tf.equal(tf.argmax(output_placeholder, axis=1), tf.argmax(output, axis=1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(accuracy, tf.float32))\n",
        "\n",
        "    test_dict = {\n",
        "        input_placeholder: input_test,\n",
        "        output_placeholder: output_test\n",
        "    }\n",
        "\n",
        "    print(\"Accuracy = {}%\".format(sess.run(accuracy, feed_dict=test_dict) * 100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch = 100, Current Error = 0.08317404240369797\n",
            "Epoch = 200, Current Error = 0.08114657551050186\n",
            "Epoch = 300, Current Error = 0.06328898668289185\n",
            "Epoch = 400, Current Error = 0.0631011575460434\n",
            "Epoch = 500, Current Error = 0.06295284628868103\n",
            "Epoch = 500, Validation Error = 0.06476254761219025\n",
            "Epoch = 600, Current Error = 0.06283047795295715\n",
            "Epoch = 700, Current Error = 0.06272728741168976\n",
            "Epoch = 800, Current Error = 0.0626390278339386\n",
            "Epoch = 900, Current Error = 0.06256256997585297\n",
            "Epoch = 1000, Current Error = 0.062495626509189606\n",
            "Epoch = 1000, Validation Error = 0.06409168988466263\n",
            "Epoch = 1100, Current Error = 0.06243639066815376\n",
            "Epoch = 1200, Current Error = 0.06238348409533501\n",
            "Epoch = 1300, Current Error = 0.0623357780277729\n",
            "Epoch = 1400, Current Error = 0.06229243054986\n",
            "Epoch = 1500, Current Error = 0.06225277855992317\n",
            "Epoch = 1500, Validation Error = 0.06367412209510803\n",
            "Epoch = 1600, Current Error = 0.06221623718738556\n",
            "Epoch = 1700, Current Error = 0.06218235567212105\n",
            "Epoch = 1800, Current Error = 0.062150806188583374\n",
            "Epoch = 1900, Current Error = 0.06212131306529045\n",
            "Epoch = 2000, Current Error = 0.062093622982501984\n",
            "Epoch = 2000, Validation Error = 0.0633702427148819\n",
            "Epoch = 2100, Current Error = 0.06206751614809036\n",
            "Epoch = 2200, Current Error = 0.06204284355044365\n",
            "Epoch = 2300, Current Error = 0.06201948598027229\n",
            "Epoch = 2400, Current Error = 0.06199733912944794\n",
            "Epoch = 2500, Current Error = 0.06197628006339073\n",
            "Epoch = 2500, Validation Error = 0.06313032656908035\n",
            "Epoch = 2600, Current Error = 0.06195622682571411\n",
            "Epoch = 2700, Current Error = 0.06193709000945091\n",
            "Epoch = 2800, Current Error = 0.061918847262859344\n",
            "Epoch = 2900, Current Error = 0.06190139055252075\n",
            "Epoch = 3000, Current Error = 0.06188470497727394\n",
            "Epoch = 3000, Validation Error = 0.06293331831693649\n",
            "Epoch = 3100, Current Error = 0.06186871603131294\n",
            "Epoch = 3200, Current Error = 0.06185341998934746\n",
            "Epoch = 3300, Current Error = 0.06183871626853943\n",
            "Epoch = 3400, Current Error = 0.06182461604475975\n",
            "Epoch = 3500, Current Error = 0.061811115592718124\n",
            "Epoch = 3500, Validation Error = 0.06276829540729523\n",
            "Epoch = 3600, Current Error = 0.0617980919778347\n",
            "Epoch = 3700, Current Error = 0.06178560107946396\n",
            "Epoch = 3800, Current Error = 0.061773572117090225\n",
            "Epoch = 3900, Current Error = 0.061761997640132904\n",
            "Epoch = 4000, Current Error = 0.061750855296850204\n",
            "Epoch = 4000, Validation Error = 0.06262839585542679\n",
            "Epoch = 4100, Current Error = 0.06174010783433914\n",
            "Epoch = 4200, Current Error = 0.06172975152730942\n",
            "Epoch = 4300, Current Error = 0.061719752848148346\n",
            "Epoch = 4400, Current Error = 0.061710089445114136\n",
            "Epoch = 4500, Current Error = 0.061700765043497086\n",
            "Epoch = 4500, Validation Error = 0.0625087097287178\n",
            "Epoch = 4600, Current Error = 0.0616917684674263\n",
            "Epoch = 4700, Current Error = 0.061683062463998795\n",
            "Epoch = 4800, Current Error = 0.06167462840676308\n",
            "Epoch = 4900, Current Error = 0.06166648492217064\n",
            "Epoch = 5000, Current Error = 0.0616585873067379\n",
            "Epoch = 5000, Validation Error = 0.06240559741854668\n",
            "Accuracy = 51.875001192092896%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URrgVgbSid9r"
      },
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfJHX1has9T8"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSQ7Vb3mxGy2",
        "outputId": "699c797f-d421-46b3-a50d-79bf61d69b7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    accuracy = tf.equal(tf.argmax(output_placeholder, axis=1), tf.argmax(output, axis=1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(accuracy, tf.float32))\n",
        "\n",
        "    test_dict = {\n",
        "        input_placeholder: input_test,\n",
        "        output_placeholder: output_test\n",
        "    }\n",
        "\n",
        "    print(sess.run(tf.argmax(output_placeholder, axis=1), feed_dict=test_dict))\n",
        "    print(sess.run(tf.argmax(output, axis=1), feed_dict=test_dict))\n",
        "    print(sess.run(tf.equal(tf.argmax(output_placeholder, axis=1), tf.argmax(output, axis=1)), feed_dict=test_dict))\n",
        "    print(sess.run(accuracy, feed_dict=test_dict))\n",
        "    # print(\"Accuracy = {}%\".format(sess.run(accuracy, feed_dict=test_dict) * 100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4 0 2 3 2 2 2 4 0 3 0 0 1 2 0 0 2 0 2 0 2 1 2 2 2 0 0 0 0 0 0 0 1 0 0 2 2\n",
            " 2 0 0 2 2 2 0 2 0 3 0 0 3 0 2 3 0 0 0 0 2 0 0 3 2 2 3 0 2 2 2 0 2 2 2 0 3\n",
            " 0 3 2 2 0 2 0 0 2 2 0 3 0 0 0 0 0 0 0 2 2 2 0 0 3 2 4 0 0 3 0 2 0 0 2 0 2\n",
            " 0 0 2 0 2 0 2 2 2 2 0 0 1 3 2 0 0 3 2 2 0 2 0 2 2 0 2 0 2 0 0 2 2 1 0 0 2\n",
            " 2 3 0 0 0 3 3 4 0 0 0 2]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[False  True False False False False False False  True False  True  True\n",
            " False False  True  True False  True False  True False False False False\n",
            " False  True  True  True  True  True  True  True False  True  True False\n",
            " False False  True  True False False False  True False  True False  True\n",
            "  True False  True False False  True  True  True  True False  True  True\n",
            " False False False False  True False False False  True False False False\n",
            "  True False  True False False False  True False  True  True False False\n",
            "  True False  True  True  True  True  True  True  True False False False\n",
            "  True  True False False False  True  True False  True False  True  True\n",
            " False  True False  True  True False  True False  True False False False\n",
            " False  True  True False False False  True  True False False False  True\n",
            " False  True False False  True False  True False  True  True False False\n",
            " False  True  True False False False  True  True  True False False False\n",
            "  True  True  True False]\n",
            "0.46875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWj2ls0FuXQ8"
      },
      "source": [
        "# Clustering (Self-Organizing Map)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnOn2rhaugD_",
        "outputId": "50480b24-8f25-402e-cf61-73ed14178617",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "data_clustering = pd.read_csv('E202-COMP7117-TD01-00 - clustering.csv')\n",
        "print(data_clustering)\n",
        " \n",
        "# Feature Selection\n",
        "new_clustering = pd.DataFrame(columns = ['Special Day Rate', 'Visitor Type', 'Weekend', 'Product Related Duration', 'Exit Rates'])\n",
        " \n",
        "new_clustering['Special Day Rate'] = data_clustering['SpecialDay'].map({'LOW': 0, 'NORMAL': 1, 'HIGH': 2})\n",
        "new_clustering['Visitor Type'] = data_clustering['VisitorType'].map({'Other': 0, 'New_Visitor': 1, 'Returning_Visitor': 2})\n",
        "new_clustering['Weekend'] = data_clustering['Weekend'].map({False: 0, True: 1})\n",
        "new_clustering['Product Related Duration'] = data_clustering['ProductRelated_Duration']\n",
        "new_clustering['Exit Rates'] = data_clustering['ExitRates']\n",
        " \n",
        "print(new_clustering)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Administrative  Administrative_Duration  ...  Weekend  Revenue\n",
            "0                  0                      0.0  ...    False    False\n",
            "1                  0                      0.0  ...    False    False\n",
            "2                  0                     -1.0  ...    False    False\n",
            "3                  0                      0.0  ...    False    False\n",
            "4                  0                      0.0  ...     True    False\n",
            "...              ...                      ...  ...      ...      ...\n",
            "3627               0                      0.0  ...    False    False\n",
            "3628               0                      0.0  ...    False    False\n",
            "3629               0                      0.0  ...    False    False\n",
            "3630               3                     18.0  ...    False    False\n",
            "3631               0                      0.0  ...    False    False\n",
            "\n",
            "[3632 rows x 18 columns]\n",
            "      Special Day Rate  Visitor Type  ...  Product Related Duration  Exit Rates\n",
            "0                    0             2  ...                         0           3\n",
            "1                    0             2  ...                         0           3\n",
            "2                    0             2  ...                         0           3\n",
            "3                    0             2  ...                         0           3\n",
            "4                    0             2  ...                         1           2\n",
            "...                ...           ...  ...                       ...         ...\n",
            "3627                 0             0  ...                         0           3\n",
            "3628                 0             0  ...                         0           3\n",
            "3629                 0             0  ...                         3           2\n",
            "3630                 0             0  ...                         0           2\n",
            "3631                 0             0  ...                         3           3\n",
            "\n",
            "[3632 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1Rc6nxTf3e9",
        "outputId": "4011cd1b-8856-4e56-a248-86c738a8c5ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "# Feature Extraction\n",
        "\n",
        "def calc_mean(dataset):\n",
        "    return tf.reduce_mean(dataset, axis = 0)\n",
        "\n",
        "def normalize_data(dataset, mean):\n",
        "    return dataset - mean\n",
        "\n",
        "def apply_pca(normalized_dataset, n):\n",
        "    pca = PCA(n_components = n)\n",
        "    pca = pca.fit(normalized_dataset)\n",
        "    return pca.transform(normalized_dataset)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    clustering_mean = sess.run(calc_mean(new_clustering))\n",
        "    \n",
        "clustering_norm = normalize_data(new_clustering, clustering_mean)\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# scaler.fit(new_clustering)\n",
        "# clustering_norm = scaler.transform(new_clustering)\n",
        "\n",
        "clustering_pca = apply_pca(clustering_norm, 3)\n",
        "\n",
        "print(clustering_pca)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 2.27066853 -0.64400383 -0.54780557]\n",
            " [ 2.27066853 -0.64400383 -0.54780557]\n",
            " [ 2.27066853 -0.64400383 -0.54780557]\n",
            " ...\n",
            " [-0.66133867  0.39225062 -1.1310385 ]\n",
            " [ 1.51046058 -1.34116664 -0.00537215]\n",
            " [ 0.01647496  0.89114655 -1.66497783]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7_lYIUPocvu",
        "outputId": "da633608-ca8a-42a5-d7a4-84482ea79b7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        }
      },
      "source": [
        "class SOM:\n",
        "    def __init__(self, width, height, input_dimension):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.input_dimension = input_dimension\n",
        "\n",
        "        self.weight = tf.Variable(tf.random_normal([width * height, input_dimension]))\n",
        "        self.input = tf.placeholder(tf.float32, [input_dimension])\n",
        "\n",
        "        self.location = [tf.to_float([y,x]) for y in range(height) for x in range(width)]\n",
        "\n",
        "        self.bmu = self.getBMU()\n",
        "\n",
        "        self.update_weight = self.update_neigbours()\n",
        "\n",
        "    def getBMU(self):\n",
        "        square_distance = tf.square(self.input - self.weight)\n",
        "        distance = tf.sqrt(tf.reduce_sum(square_distance, axis=1))\n",
        "\n",
        "        bmu_index = tf.argmin(distance)\n",
        "\n",
        "        bmu_position = tf.to_float([tf.div(bmu_index,self.width), tf.mod(bmu_index, self.width)])\n",
        "        return bmu_position\n",
        "\n",
        "    def update_neigbours(self):\n",
        "        learning_rate = 0.1\n",
        "\n",
        "        sigma = tf.to_float(tf.maximum(self.width, self.height) / 2)\n",
        "\n",
        "        square_difference = tf.square(self.bmu - self.location)\n",
        "        distance = tf.sqrt(tf.reduce_sum(square_difference,axis=1))\n",
        "\n",
        "        NS = tf.exp(tf.div(tf.negative(tf.square(distance)), 2 * tf.square(sigma)))\n",
        "\n",
        "        rate = NS * learning_rate\n",
        "\n",
        "        rate_stacked = tf.stack([tf.tile(tf.slice(rate,[i],[1]), [self.input_dimension]) \n",
        "            for i in range(self.width * self.height)])\n",
        "\n",
        "        new_weight = self.weight + rate_stacked * (self.input - self.weight)\n",
        "\n",
        "        return tf.assign(self.weight, new_weight)\n",
        "\n",
        "    def train(self, dataset, epoch):\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "\n",
        "            for i in range(epoch+1):\n",
        "                if (i%1 == 0) :\n",
        "                  print(i)\n",
        "                for data in dataset:\n",
        "                    dictionary = {\n",
        "                        self.input : data\n",
        "                    }\n",
        "\n",
        "                    sess.run(self.update_weight,feed_dict=dictionary)\n",
        "\n",
        "            location = sess.run(self.location)\n",
        "            weight = sess.run(self.weight)\n",
        "\n",
        "            clusters = [[] for i in range(self.height)]\n",
        "\n",
        "            for i, loc in enumerate(location):\n",
        "                clusters[int(loc[0])].append(weight[i])\n",
        "\n",
        "            self.clusters = clusters\n",
        "\n",
        "\n",
        "\n",
        "input_dimension = len(clustering_pca[0])\n",
        "# epoch = 5000\n",
        "epoch = 5 # TODO: epochnya balikin ke 5000\n",
        "\n",
        "som = SOM(3,3,input_dimension)\n",
        "\n",
        "som.train(clustering_pca,epoch)\n",
        "plt.imshow(som.clusters)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-63-2f839789c1d0>:10: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From <ipython-input-63-2f839789c1d0>:22: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtfTOhVh2JKY"
      },
      "source": [
        "# Classification (yg sdh rapi)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo9bb-ef2LqM",
        "outputId": "f7d29967-bcde-475e-ea90-512a3539ceea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "data_classification = pd.read_csv('E202-COMP7117-TD01-00 - classification.csv')\n",
        "\n",
        "\n",
        "# Feature Selection\n",
        "new_classification = pd.DataFrame(columns = ['Volatile Acidity', 'Chlorides', 'Free Sulfur Dioxide', 'Total Sulfur Dioxide', 'Density', 'pH', 'Sulphates', 'Alcohol'])\n",
        "new_classification['Volatile Acidity'] = data_classification['volatile acidity']\n",
        "new_classification['Chlorides'] = data_classification['chlorides']\n",
        "new_classification['Free Sulfur Dioxide'] = data_classification['free sulfur dioxide'].map({'Low': 1, 'Medium': 2, 'High': 3}).fillna(0).astype(int)\n",
        "new_classification['Total Sulfur Dioxide'] = data_classification['total sulfur dioxide']\n",
        "new_classification['Density'] = data_classification['density'].map({'Very High': 0, 'Low':1, 'Medium': 2, 'High': 3})\n",
        "new_classification['pH'] = data_classification['pH'].map({'Very Acidic': 1, 'Normal': 2, 'Very Basic': 3}).fillna(0).astype(int)\n",
        "new_classification['Sulphates'] = data_classification['sulphates']\n",
        "new_classification['Alcohol'] = data_classification['alcohol']\n",
        "\n",
        "\n",
        "# Feature Extraction\n",
        "with tf.Session() as sess:\n",
        "    classification_mean = sess.run(calc_mean(new_classification))\n",
        "classification_norm = normalize_data(new_classification, classification_mean)\n",
        "\n",
        "classification_pca = apply_pca(classification_norm, 4)\n",
        "\n",
        "# input_data = MinMaxScaler().fit_transform(classification_pca)\n",
        "input_data = MinMaxScaler().fit_transform(new_classification)\n",
        "output_data = OneHotEncoder(sparse=False).fit_transform(data_classification[['quality']])\n",
        "\n",
        "\n",
        "# Training\n",
        "layer = {\n",
        "    # \"input\": 4,\n",
        "    \"input\": 8,\n",
        "    \"hidden\": 30,\n",
        "    \"output\": 5\n",
        "}\n",
        "\n",
        "weight = {\n",
        "    \"input_to_hidden\": tf.Variable(tf.random_normal([layer[\"input\"], layer[\"hidden\"]])),\n",
        "    \"hidden_to_output\": tf.Variable(tf.random_normal([layer[\"hidden\"], layer[\"output\"]]))\n",
        "}\n",
        "\n",
        "bias = {\n",
        "    \"input_to_hidden\": tf.Variable(tf.random_normal([layer[\"hidden\"]])),\n",
        "    \"hidden_to_output\": tf.Variable(tf.random_normal([layer[\"output\"]]))\n",
        "}\n",
        "\n",
        "input_placeholder = tf.placeholder(tf.float32, [None, layer[\"input\"]])\n",
        "output_placeholder = tf.placeholder(tf.float32, [None, layer[\"output\"]])\n",
        "\n",
        "def feed_forward(data):\n",
        "    y = tf.matmul(data, weight[\"input_to_hidden\"]) + bias[\"input_to_hidden\"]\n",
        "    y2 = tf.nn.sigmoid(y)\n",
        "\n",
        "    z = tf.matmul(y2, weight[\"hidden_to_output\"]) + bias[\"hidden_to_output\"]\n",
        "    z2 = tf.nn.sigmoid(z)\n",
        "\n",
        "    return z2\n",
        "\n",
        "output = feed_forward(input_placeholder)\n",
        "\n",
        "epoch = 5000\n",
        "alpha = 0.9\n",
        "\n",
        "loss = tf.reduce_mean(0.5 * (output_placeholder - output) ** 2)\n",
        "optimizer = tf.train.GradientDescentOptimizer(alpha)\n",
        "train = optimizer.minimize(loss)\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "prev_validation_error = 0\n",
        "\n",
        "input_train, input_validation, output_train, output_validation = train_test_split(input_data, output_data, test_size=0.2)\n",
        "input_train, input_test, output_train, output_test = train_test_split(input_train, output_train, test_size=0.125)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for i in range(1, epoch+1):\n",
        "\n",
        "        train_dict = {\n",
        "            input_placeholder: input_train,\n",
        "            output_placeholder: output_train\n",
        "        }\n",
        "\n",
        "        sess.run(train, feed_dict=train_dict)\n",
        "        error_train = sess.run(loss, feed_dict=train_dict)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(\"Epoch = {}, Current Error = {}\".format(i, error_train))\n",
        "\n",
        "        if i % 500 == 0:\n",
        "            validation_dict = {\n",
        "                input_placeholder: input_validation,\n",
        "                output_placeholder: output_validation\n",
        "            }\n",
        "\n",
        "            error_validation = sess.run(loss, feed_dict=validation_dict)\n",
        "\n",
        "            print(\"Epoch = {}, Validation Error = {}\".format(i, error_validation))\n",
        "\n",
        "            if i == 500 or error_validation < prev_validation_error:\n",
        "                prev_validation_error = error_validation\n",
        "                saver.save(sess, \"model/model.ckpt\")\n",
        "\n",
        "    accuracy = tf.equal(tf.argmax(output_placeholder, axis=1), tf.argmax(output, axis=1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(accuracy, tf.float32))\n",
        "\n",
        "    test_dict = {\n",
        "        input_placeholder: input_test,\n",
        "        output_placeholder: output_test\n",
        "    }\n",
        "\n",
        "    print(\"Accuracy = {}%\".format(sess.run(accuracy, feed_dict=test_dict) * 100))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c977f1a15780>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Feature Extraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mclassification_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalc_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_classification\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mclassification_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_classification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'calc_mean' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj2Qytol43XN"
      },
      "source": [
        "# Classification (coba-coba pake library)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-5axwqp42vD",
        "outputId": "c4222b94-de0b-440c-fca2-222dc9185b0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        " \n",
        "# new_classification2 = StandardScaler().fit_transform(new_classification)\n",
        "# classification_pca = apply_pca(new_classification2, 4)\n",
        "\n",
        "# input_data = MinMaxScaler().fit_transform(classification_pca)\n",
        "\n",
        "# X = input_data\n",
        "X = classification_norm.values\n",
        "y = output_data\n",
        " \n",
        "folds = 5\n",
        "kf = KFold(n_splits=folds)\n",
        "kf.get_n_splits(X)\n",
        " \n",
        "knn = [0, 0, 0, 0]\n",
        "rf = [0, 0, 0, 0]\n",
        " \n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        " \n",
        "    modelknn = KNeighborsClassifier(n_neighbors=5).fit(X_train, y_train)\n",
        "    modelrf = RandomForestClassifier(max_depth=None,random_state=None).fit(X_train,y_train)\n",
        " \n",
        "    predictknn = modelknn.predict(X_test)\n",
        "    predictrf = modelrf.predict(X_test)\n",
        "\n",
        "    # print(predictknn, y_test)\n",
        " \n",
        "    knn[0] += accuracy_score(y_test, predictknn) * 100\n",
        "    knn[1] += precision_score(y_test, predictknn, average='weighted') * 100\n",
        "    knn[2] += recall_score(y_test, predictknn, average='weighted') * 100\n",
        "    knn[3] += f1_score(y_test, predictknn, average='weighted')\n",
        " \n",
        "    rf[0] += accuracy_score(y_test, predictrf) * 100\n",
        "    rf[1] += precision_score(y_test, predictrf, average='weighted') * 100\n",
        "    rf[2] += recall_score(y_test, predictrf, average='weighted') * 100\n",
        "    rf[3] += f1_score(y_test, predictrf, average='weighted')\n",
        " \n",
        "print(\"KNN:\")\n",
        "print(\"Accuracy: \", knn[0] / folds)\n",
        "print(\"Precision: \", knn[1] / folds)\n",
        "print(\"Recall: \", knn[2] / folds)\n",
        "print(\"F1 Score: \", knn[3] / folds)\n",
        "\n",
        "print(\"\\nRF:\")\n",
        "print(\"Accuracy: \", rf[0] / folds)\n",
        "print(\"Precision: \", rf[1] / folds)\n",
        "print(\"Recall: \", rf[2] / folds)\n",
        "print(\"F1 Score: \", rf[3] / folds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "KNN:\n",
            "Accuracy:  41.52703761755485\n",
            "Precision:  47.72433655240053\n",
            "Recall:  41.52703761755485\n",
            "F1 Score:  0.437437170864518\n",
            "\n",
            "RF:\n",
            "Accuracy:  44.593260188087775\n",
            "Precision:  58.44177824712473\n",
            "Recall:  44.593260188087775\n",
            "F1 Score:  0.4952370111512039\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVeQmyM5u89W"
      },
      "source": [
        "# Classification (kotor)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoXQ3BHOu9Ia",
        "outputId": "4b82873c-e467-4db7-90ca-2c4209e79f83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "data_classification = pd.read_csv('E202-COMP7117-TD01-00 - classification.csv')\n",
        "\n",
        "# Feature Selection\n",
        "new_classification = pd.DataFrame(columns = ['Volatile Acidity', 'Chlorides', 'Free Sulfur Dioxide', 'Total Sulfur Dioxide', 'Density', 'pH', 'Sulphates', 'Alcohol'])\n",
        "new_classification['Volatile Acidity'] = data_classification['volatile acidity']\n",
        "new_classification['Chlorides'] = data_classification['chlorides']\n",
        "new_classification['Free Sulfur Dioxide'] = data_classification['free sulfur dioxide'].map({'Low': 1, 'Medium': 2, 'High': 3}).fillna(0).astype(int)\n",
        "new_classification['Total Sulfur Dioxide'] = data_classification['total sulfur dioxide']\n",
        "new_classification['Density'] = data_classification['density'].map({'Very High': 0, 'Low':1, 'Medium': 2, 'High': 3})\n",
        "new_classification['pH'] = data_classification['pH'].map({'Very Acidic': 1, 'Normal': 2, 'Very Basic': 3}).fillna(0).astype(int)\n",
        "new_classification['Sulphates'] = data_classification['sulphates']\n",
        "new_classification['Alcohol'] = data_classification['alcohol']\n",
        "\n",
        "# Output\n",
        "#ini outputnya ga di suruh di map sih tapi seharusnya di map kan ya?\n",
        "# new_classification['Quality'] = data_classification['quality'].map({'Fair': 1, 'Decent': 2, 'Fine': 3, 'Good': 4, 'Great': 5})\n",
        "\n",
        "print(new_classification)\n",
        "print(new_classification.isna())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4d571d5e7d97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_classification\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'E202-COMP7117-TD01-00 - classification.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Feature Selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnew_classification\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Volatile Acidity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Chlorides'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Free Sulfur Dioxide'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Total Sulfur Dioxide'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Density'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pH'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Sulphates'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Alcohol'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnew_classification\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Volatile Acidity'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_classification\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'volatile acidity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHrJvtIOkNwP",
        "outputId": "0eee68a6-fa7c-41aa-fe1d-0f4333cd2b4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "# Feature Extraction\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    classification_mean = sess.run(calc_mean(new_classification))\n",
        "classification_norm = normalize_data(new_classification, classification_mean)\n",
        "\n",
        "# scaler.fit(new_classification)\n",
        "# classification_norm = scaler.transform(new_classification)\n",
        "\n",
        "classification_pca = apply_pca(classification_norm, 4)\n",
        "\n",
        "# print(classification_pca)\n",
        "# print(classification_pca.shape)\n",
        "\n",
        "input_data = MinMaxScaler().fit_transform(classification_pca)\n",
        "output_data = OneHotEncoder(sparse=False).fit_transform(data_classification[['quality']])\n",
        "\n",
        "print(input_data)\n",
        "# print(input_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.04529625e-10 2.68649490e-03 9.89339135e-02 8.62583854e-01]\n",
            " [1.46341485e-10 2.53367939e-03 2.15590945e-01 6.31969093e-01]\n",
            " [1.46341480e-10 2.55323989e-03 1.69587958e-01 9.48752533e-01]\n",
            " ...\n",
            " [2.71777014e-10 2.47124871e-03 1.20169828e-01 4.32851342e-01]\n",
            " [1.88153322e-10 2.54789515e-03 1.34302659e-01 2.81493787e-01]\n",
            " [2.71777015e-10 2.28538746e-03 1.27162769e-01 5.72157888e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V546OO7AzTYA",
        "outputId": "0172246b-3793-4472-ee24-3454deb5e058",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "classification_pca2 = apply_pca(classification_norm, 2)\n",
        "input_data2 = MinMaxScaler().fit_transform(classification_pca2)\n",
        "# print(input_data2)\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "  \n",
        "x = [i[0] for i in input_data2]\n",
        "y = [i[1] for i in input_data2]\n",
        "\n",
        "plt.scatter(x, y, s=10)\n",
        "plt.show() \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAPi0lEQVR4nO3df4xlZX3H8feH/aE0RW26Ixp2dTEdjBM1gU6oraHSCHXZJrtpbHW3Ia0NkahFm2hMMDbU4B/VkpqUZFvdtsZiIoj+YSdxzQbtGohlLUNWEYbijivKosKIljQR3AW//eNeyGWYmXuHvTOX++z7lUxyz3kezvk+e+d+OPOce85JVSFJGn9njLoASdJwGOiS1AgDXZIaYaBLUiMMdElqxMZR7XjLli21ffv2Ue1eksbSnXfe+dOqmliqbWSBvn37dmZnZ0e1e0kaS0l+sFybUy6S1AgDXZIaYaBLUiMMdElqhIEuSY3oG+hJPp3k4SR3L9OeJNcnmU9yV5ILhl+mJKmfQY7QPwPsWKH9MmCy+3Ml8M+nXtbybpl7iGv+425umXtoLXcjSWOnb6BX1a3Az1boshu4oToOAy9J8vJhFdjrlrmHeN+NR7jh9h/wvhuPGOqS1GMYc+jnAA/0LB/vrnuWJFcmmU0yu7CwsOod3XZ0gcdOPgnAYyef5Lajq9+GJLVqXU+KVtX+qpququmJiSWvXF3RRZMTnLlpAwBnbtrARZOr34YktWoYl/4/CGzrWd7aXTd0l06dzfV7z+e2owtcNDnBpVNnr8VuJGksDSPQZ4CrktwE/A7waFX9eAjbXdKlU2cb5JK0hL6BnuRG4GJgS5LjwN8CmwCq6pPAAWAnMA/8AvjLtSpWkrS8voFeVXv7tBfwV0OrSJL0nHilqCQ1wkCXpEYY6JLUCANdkhoxskfQPVe3zD3k99AlaQljdYTuvVwkaXljFejey0WSljdWge69XCRpeWM1h+69XCRpeWMV6OC9XCRpOWM15SJJWp6BLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhoxdt9D9+ZckrS0sTpC9+ZckrS8sQp0b84lScsbq0A/64WbVlyWpNPZWAX6/z1+csVlSTqdjVWgXzQ5weYNnZI3bzjD2+dKUo+xCnRJ0vLGKtBvO7rAiSd/BcCJJ3/lSVFJ6jFWge4TiyRpeWN1YZFPLJKk5Y1VoINPLJKk5YzVlIskaXkGuiQ1wkCXpEYMFOhJdiS5L8l8kquXaH9FkkNJjiS5K8nO4ZcqSVpJ30BPsgHYB1wGTAF7k0wt6vY3wM1VdT6wB/inYRcqSVrZIEfoFwLzVXWsqk4ANwG7F/Up4EXd1y8GfjS8EiVJgxgk0M8BHuhZPt5d1+sjwOVJjgMHgPcutaEkVyaZTTK7sOBVnpI0TMM6KboX+ExVbQV2Ap9N8qxtV9X+qpququmJCa/ylKRhGiTQHwS29Sxv7a7rdQVwM0BV3Q68ENgyjAIlSYMZJNDvACaTnJtkM52TnjOL+vwQeDNAktfQCXTnVCRpHfUN9Kp6ArgKOAjcS+fbLPckuTbJrm63DwDvTPJt4EbgHVVVa1W0JOnZBrqXS1UdoHOys3fdNT2v54A3Drc0SdJqeKWoJDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJasRAgZ5kR5L7kswnuXqZPm9LMpfkniSfG26ZkqR+NvbrkGQDsA+4FDgO3JFkpqrmevpMAh8C3lhVP0/y0rUqWJK0tEGO0C8E5qvqWFWdAG4Cdi/q805gX1X9HKCqHh5umZKkfgYJ9HOAB3qWj3fX9ToPOC/JN5IcTrJjqQ0luTLJbJLZhYWF51axJGlJwzopuhGYBC4G9gL/kuQliztV1f6qmq6q6YmJiSHtWpIEgwX6g8C2nuWt3XW9jgMzVXWyqr4PfJdOwEuS1skggX4HMJnk3CSbgT3AzKI+X6JzdE6SLXSmYI4NsU5JUh99A72qngCuAg4C9wI3V9U9Sa5Nsqvb7SDwSJI54BDwwap6ZK2KliQ9W6pqJDuenp6u2dnZkexbksZVkjuranqpNq8UlaRGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQMFepIdSe5LMp/k6hX6vTVJJZkeXomSpEH0DfQkG4B9wGXAFLA3ydQS/c4C/hr45rCLlCT1N8gR+oXAfFUdq6oTwE3A7iX6fRT4OPD4EOuTJA1okEA/B3igZ/l4d93TklwAbKuqL6+0oSRXJplNMruwsLDqYiVJyzvlk6JJzgA+AXygX9+q2l9V01U1PTExcaq7liT1GCTQHwS29Sxv7a57ylnAa4GvJ7kfeAMw44lRSVpfgwT6HcBkknOTbAb2ADNPNVbVo1W1paq2V9V24DCwq6pm16RiSdKS+gZ6VT0BXAUcBO4Fbq6qe5Jcm2TXWhcoSRrMxkE6VdUB4MCiddcs0/fiUy9LkrRaXikqSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGjFQoCfZkeS+JPNJrl6i/f1J5pLcleRrSV45/FIlSSvpG+hJNgD7gMuAKWBvkqlF3Y4A01X1euCLwN8Pu1BJ0soGOUK/EJivqmNVdQK4Cdjd26GqDlXVL7qLh4Gtwy1TktTPIIF+DvBAz/Lx7rrlXAF8ZamGJFcmmU0yu7CwMHiVkqS+hnpSNMnlwDRw3VLtVbW/qqaranpiYmKYu5ak097GAfo8CGzrWd7aXfcMSS4BPgy8qap+OZzyJEmDGuQI/Q5gMsm5STYDe4CZ3g5Jzgc+BeyqqoeHX6YkqZ++gV5VTwBXAQeBe4Gbq+qeJNcm2dXtdh3w68AXknwrycwym5MkrZFBplyoqgPAgUXrrul5fcmQ65IkrZJXikpSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1IiNg3RKsgP4R2AD8K9V9bFF7S8AbgB+G3gEeHtV3T/cUju2X/3lp1/f/7E/WotdrOi6g/fx1bmfcMnUy/jgW17NLXMPcdvRBS6anODSqbOf0XelNkntWM1nfS1zIVW1codkA/Bd4FLgOHAHsLeq5nr6vAd4fVW9K8ke4I+r6u0rbXd6erpmZ2dXVWxvmD9lPUP9uoP3se/Q/NPLO1/3cg79z8M8dvJJzty0gev3nv/0G3TL3EO878YjS7ZJasdqPuvDyIUkd1bV9FJtg0y5XAjMV9WxqjoB3ATsXtRnN/Dv3ddfBN6cJKuqcgx8de4nz1j+r/kFHjv5JACPnXyS244uPN1229Hl2yS1YzWf9bXOhUEC/RzggZ7l4911S/apqieAR4HfXLyhJFcmmU0yu7AwfgF3ydTLnrH8e781wZmbNgBw5qYNXDQ58XTbRZPLt0lqx2o+62udCwPNoQ9LVe0H9kNnymU99z0MH3zLqwEGmkO/dOpsrt97vnPoUuNW81lf61wYZA79d4GPVNVbussfAqiqv+vpc7Db5/YkG4GfABO1wsafyxw6jP6kqCSN0kpz6IMcod8BTCY5F3gQ2AP82aI+M8BfALcDfwL850phfioMcUlaWt9Ar6onklwFHKTztcVPV9U9Sa4FZqtqBvg34LNJ5oGf0Ql9SdI6GmgOvaoOAAcWrbum5/XjwJ8OtzRJ0mp4pagkNcJAl6RGGOiS1AgDXZIa0fd76Gu242QB+MFz/M+3AD8dYjnjwDGfHhzz6eFUxvzKqlryEtORBfqpSDK73BfrW+WYTw+O+fSwVmN2ykWSGmGgS1IjxjXQ94+6gBFwzKcHx3x6WJMxj+UcuiTp2cb1CF2StIiBLkmNeF4HepIdSe5LMp/k6iXaX5Dk8932bybZvv5VDtcAY35/krkkdyX5WpJXjqLOYeo35p5+b01SScb+K26DjDnJ27rv9T1JPrfeNQ7bAL/br0hyKMmR7u/3zlHUOSxJPp3k4SR3L9OeJNd3/z3uSnLBKe+0qp6XP3Ru1fs94FXAZuDbwNSiPu8BPtl9vQf4/KjrXocx/wHwa93X7z4dxtztdxZwK3AYmB513evwPk8CR4Df6C6/dNR1r8OY9wPv7r6eAu4fdd2nOObfBy4A7l6mfSfwFSDAG4Bvnuo+n89H6Kfjw6n7jrmqDlXVL7qLh4Gt61zjsA3yPgN8FPg48Ph6FrdGBhnzO4F9VfVzgKp6eJ1rHLZBxlzAi7qvXwz8aB3rG7qqupXO8yGWsxu4oToOAy9J8vJT2efzOdCH9nDqMTLImHtdQef/8OOs75i7f4puq6ov04ZB3ufzgPOSfCPJ4SQ71q26tTHImD8CXJ7kOJ3nL7x3fUobmdV+3vta14dEa3iSXA5MA28adS1rKckZwCeAd4y4lPW2kc60y8V0/gq7Ncnrqup/R1rV2toLfKaq/qH7LOPPJnltVf1q1IWNi+fzEfqDwLae5a3ddUv26T6c+sXAI+tS3doYZMwkuQT4MLCrqn65TrWtlX5jPgt4LfD1JPfTmWucGfMTo4O8z8eBmao6WVXfB75LJ+DH1SBjvgK4GaCqbgdeSOcmVq0a6PO+Gs/nQH/64dRJNtM56TmzqM9TD6eGNX449TrpO+Yk5wOfohPm4z6vCn3GXFWPVtWWqtpeVdvpnDfYVVWzoyl3KAb53f4SnaNzkmyhMwVzbD2LHLJBxvxD4M0ASV5DJ9AX1rXK9TUD/Hn32y5vAB6tqh+f0hZHfSa4z1ninXSOTL4HfLi77lo6H2jovOFfAOaB/wZeNeqa12HMXwUeAr7V/ZkZdc1rPeZFfb/OmH/LZcD3OXSmmuaA7wB7Rl3zOox5CvgGnW/AfAv4w1HXfIrjvRH4MXCSzl9cVwDvAt7V8x7v6/57fGcYv9de+i9JjXg+T7lIklbBQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmN+H/2Lw3iR+/rQAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCi9xVz4rm5r",
        "outputId": "2aa41169-17bf-46da-cc08-46ff0e4bf1c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "layer = {\n",
        "    \"input\": 4,\n",
        "    \"hidden\": 30,\n",
        "    \"output\": 5\n",
        "}\n",
        "\n",
        "weight = {\n",
        "    \"input_to_hidden\": tf.Variable(tf.random_normal([layer[\"input\"], layer[\"hidden\"]])),\n",
        "    \"hidden_to_output\": tf.Variable(tf.random_normal([layer[\"hidden\"], layer[\"output\"]]))\n",
        "}\n",
        "\n",
        "bias = {\n",
        "    \"input_to_hidden\": tf.Variable(tf.random_normal([layer[\"hidden\"]])),\n",
        "    \"hidden_to_output\": tf.Variable(tf.random_normal([layer[\"output\"]]))\n",
        "}\n",
        "\n",
        "input_placeholder = tf.placeholder(tf.float32, [None, layer[\"input\"]])\n",
        "output_placeholder = tf.placeholder(tf.float32, [None, layer[\"output\"]])\n",
        "\n",
        "def feed_forward(data):\n",
        "\n",
        "    y = tf.matmul(data, weight[\"input_to_hidden\"]) + bias[\"input_to_hidden\"]\n",
        "    y2 = tf.nn.sigmoid(y)\n",
        "\n",
        "    z = tf.matmul(y2, weight[\"hidden_to_output\"]) + bias[\"hidden_to_output\"]\n",
        "    z2 = tf.nn.sigmoid(z)\n",
        "\n",
        "    return z2\n",
        "\n",
        "# weight = {\n",
        "#     \"input_to_output\": tf.Variable(tf.random_normal([layer[\"input\"], layer[\"output\"]])),\n",
        "# }\n",
        "\n",
        "# bias = {\n",
        "#     \"input_to_output\": tf.Variable(tf.random_normal([layer[\"output\"]])),\n",
        "# }\n",
        "\n",
        "# input_placeholder = tf.placeholder(tf.float32, [None, layer[\"input\"]])\n",
        "# output_placeholder = tf.placeholder(tf.float32, [None, layer[\"output\"]])\n",
        "\n",
        "# def feed_forward(data):\n",
        "\n",
        "#     y = tf.matmul(data, weight[\"input_to_output\"]) + bias[\"input_to_output\"]\n",
        "#     y2 = tf.nn.sigmoid(y)\n",
        "\n",
        "#     return y2\n",
        "\n",
        "output = feed_forward(input_placeholder)\n",
        "\n",
        "epoch = 5000\n",
        "alpha = 0.9\n",
        "\n",
        "loss = tf.reduce_mean(0.5 * (output_placeholder - output) ** 2)\n",
        "optimizer = tf.train.GradientDescentOptimizer(alpha)\n",
        "train = optimizer.minimize(loss)\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "prev_validation_error = 0\n",
        "\n",
        "# semoga ini splitnya bener wkwkw\n",
        "input_train, input_validation, output_train, output_validation = train_test_split(input_data, output_data, test_size=0.2)\n",
        "input_train, input_test, output_train, output_test = train_test_split(input_train, output_train, test_size=0.125)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for i in range(1, epoch+1):\n",
        "\n",
        "        train_dict = {\n",
        "            input_placeholder: input_train,\n",
        "            output_placeholder: output_train\n",
        "        }\n",
        "\n",
        "        sess.run(train, feed_dict=train_dict)\n",
        "        error_train = sess.run(loss, feed_dict=train_dict)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(\"Current Error = {}, Epoch = {}\".format(error_train, i))\n",
        "\n",
        "        if i % 500 == 0:\n",
        "            validation_dict = {\n",
        "                input_placeholder: input_validation,\n",
        "                output_placeholder: output_validation\n",
        "            }\n",
        "\n",
        "            error_validation = sess.run(loss, feed_dict=validation_dict)\n",
        "\n",
        "            print(\"Validation Current Error = {}, Epoch = {}\".format(error_validation, i))\n",
        "\n",
        "            if i == 500:\n",
        "                prev_validation_error = error_validation\n",
        "            elif error_validation < prev_validation_error:\n",
        "                prev_validation_error = error_validation\n",
        "                saver.save(sess, \"model/model.ckpt\")\n",
        "\n",
        "    accuracy = tf.equal(tf.argmax(output_placeholder, axis=1), tf.argmax(output, axis=1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(accuracy, tf.float32))\n",
        "\n",
        "    test_dict = {\n",
        "        input_placeholder: input_test,\n",
        "        output_placeholder: output_test\n",
        "    }\n",
        "\n",
        "    print(\"Accuracy = {}%\".format(sess.run(accuracy, feed_dict=test_dict) * 100))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current Error = 0.08155761659145355, Epoch = 100\n",
            "Current Error = 0.08051542937755585, Epoch = 200\n",
            "Current Error = 0.07843859493732452, Epoch = 300\n",
            "Current Error = 0.0640651136636734, Epoch = 400\n",
            "Current Error = 0.06383925676345825, Epoch = 500\n",
            "Validation Current Error = 0.0649474710226059, Epoch = 500\n",
            "Current Error = 0.06364346295595169, Epoch = 600\n",
            "Current Error = 0.06347059458494186, Epoch = 700\n",
            "Current Error = 0.06331638991832733, Epoch = 800\n",
            "Current Error = 0.063177689909935, Epoch = 900\n",
            "Current Error = 0.06305203586816788, Epoch = 1000\n",
            "Validation Current Error = 0.06410297006368637, Epoch = 1000\n",
            "Current Error = 0.06293745338916779, Epoch = 1100\n",
            "Current Error = 0.06283239275217056, Epoch = 1200\n",
            "Current Error = 0.06273555755615234, Epoch = 1300\n",
            "Current Error = 0.06264598667621613, Epoch = 1400\n",
            "Current Error = 0.06256275624036789, Epoch = 1500\n",
            "Validation Current Error = 0.06363091617822647, Epoch = 1500\n",
            "Current Error = 0.062485285103321075, Epoch = 1600\n",
            "Current Error = 0.06241294741630554, Epoch = 1700\n",
            "Current Error = 0.062345247715711594, Epoch = 1800\n",
            "Current Error = 0.062281809747219086, Epoch = 1900\n",
            "Current Error = 0.062222275882959366, Epoch = 2000\n",
            "Validation Current Error = 0.06333424896001816, Epoch = 2000\n",
            "Current Error = 0.06216633692383766, Epoch = 2100\n",
            "Current Error = 0.06211370974779129, Epoch = 2200\n",
            "Current Error = 0.06206418573856354, Epoch = 2300\n",
            "Current Error = 0.062017545104026794, Epoch = 2400\n",
            "Current Error = 0.06197357177734375, Epoch = 2500\n",
            "Validation Current Error = 0.06313511729240417, Epoch = 2500\n",
            "Current Error = 0.061932094395160675, Epoch = 2600\n",
            "Current Error = 0.06189300864934921, Epoch = 2700\n",
            "Current Error = 0.061856042593717575, Epoch = 2800\n",
            "Current Error = 0.061821166425943375, Epoch = 2900\n",
            "Current Error = 0.061788205057382584, Epoch = 3000\n",
            "Validation Current Error = 0.06299659609794617, Epoch = 3000\n",
            "Current Error = 0.06175706908106804, Epoch = 3100\n",
            "Current Error = 0.061727624386548996, Epoch = 3200\n",
            "Current Error = 0.061699770390987396, Epoch = 3300\n",
            "Current Error = 0.061673425137996674, Epoch = 3400\n",
            "Current Error = 0.06164843589067459, Epoch = 3500\n",
            "Validation Current Error = 0.06289813667535782, Epoch = 3500\n",
            "Current Error = 0.06162476912140846, Epoch = 3600\n",
            "Current Error = 0.061602361500263214, Epoch = 3700\n",
            "Current Error = 0.061581097543239594, Epoch = 3800\n",
            "Current Error = 0.061560943722724915, Epoch = 3900\n",
            "Current Error = 0.06154176965355873, Epoch = 4000\n",
            "Validation Current Error = 0.06282704323530197, Epoch = 4000\n",
            "Current Error = 0.06152355298399925, Epoch = 4100\n",
            "Current Error = 0.06150627136230469, Epoch = 4200\n",
            "Current Error = 0.06148983910679817, Epoch = 4300\n",
            "Current Error = 0.061474189162254333, Epoch = 4400\n",
            "Current Error = 0.06145927682518959, Epoch = 4500\n",
            "Validation Current Error = 0.06277495622634888, Epoch = 4500\n",
            "Current Error = 0.06144506856799126, Epoch = 4600\n",
            "Current Error = 0.06143152341246605, Epoch = 4700\n",
            "Current Error = 0.06141860410571098, Epoch = 4800\n",
            "Current Error = 0.06140625849366188, Epoch = 4900\n",
            "Current Error = 0.06139446794986725, Epoch = 5000\n",
            "Validation Current Error = 0.06273620575666428, Epoch = 5000\n",
            "Accuracy = 52.49999761581421%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
